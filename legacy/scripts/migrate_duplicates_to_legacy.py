import os
import shutil
import re

# Path to the duplicate paragraphs report generated by your analysis scripts
DUPLICATE_REPORT = 'docs/Report/potential_duplicate_paragraphs.txt'
# Directory where legacy/archive files will be moved
LEGACY_DIR = 'legacy/'

# Helper: Parse the duplicate paragraphs report to get file groups

def parse_duplicate_report(report_path):
    groups = []
    with open(report_path, 'r', encoding='utf-8') as f:
        content = f.read()
    # Split by 'Paragraph:'
    blocks = [b for b in content.split('Paragraph:') if b.strip()]
    for block in blocks:
        files = re.findall(r'- (/.+)', block)
        if len(files) > 1:
            groups.append(files)
    return groups

# Helper: Move non-canonical files to legacy dir

def move_to_legacy(file_groups, canonical_selector=None):
    project_root = os.path.abspath(os.getcwd())
    for group in file_groups:
        # By default, keep the first file as canonical
        canonical = group[0] if not canonical_selector else canonical_selector(group)
        for f in group:
            if f != canonical:
                abs_f = os.path.abspath(f)
                # Only move files inside the project directory
                if abs_f.startswith(project_root):
                    rel_path = os.path.relpath(abs_f, start=project_root)
                    legacy_path = os.path.join(LEGACY_DIR, rel_path)
                    os.makedirs(os.path.dirname(legacy_path), exist_ok=True)
                    shutil.move(abs_f, legacy_path)
                    print(f"Moved {f} -> {legacy_path}")
                else:
                    print(f"Skipping {f} (outside project directory)")

# Helper: Update references in all markdown files

def update_references(root_dir, old_path, new_path):
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.md'):
                file_path = os.path.join(subdir, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    try:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            content = f.read()
                    except Exception as e:
                        print(f"Skipping {file_path} due to encoding error: {e}")
                        continue
                updated = content.replace(old_path, new_path)
                if updated != content:
                    try:
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(updated)
                    except UnicodeEncodeError:
                        with open(file_path, 'w', encoding='latin-1') as f:
                            f.write(updated)
                    print(f"Updated references in {file_path}")

if __name__ == '__main__':
    # 1. Parse duplicate report
    file_groups = parse_duplicate_report(DUPLICATE_REPORT)
    # 2. Move non-canonical files to legacy
    move_to_legacy(file_groups)
    # 3. Update references in docs and codebase
    for group in file_groups:
        canonical = group[0]
        for f in group:
            if f != canonical:
                update_references('.', f, canonical)

    print("Duplicate migration and reference update complete.")
